{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1702659418164,
     "user": {
      "displayName": "Olivier Jaylet",
      "userId": "17139053766533794240"
     },
     "user_tz": -60
    },
    "id": "a_MIX4UeGr6G"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from time import sleep\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clear the books url (delete '/../../')\n",
    "\n",
    "def clean_url(url):\n",
    "    \n",
    "    # Split the path by '/'\n",
    "    path_parts = url.split('/')\n",
    "    cleaned_path_parts = [part for part in path_parts if part and part != '..']\n",
    "\n",
    "    # Join the cleaned path parts back together with '/'\n",
    "    cleaned_link = '/'.join(cleaned_path_parts)\n",
    "\n",
    "    return cleaned_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1444,
     "status": "ok",
     "timestamp": 1702659985414,
     "user": {
      "displayName": "Olivier Jaylet",
      "userId": "17139053766533794240"
     },
     "user_tz": -60
    },
    "id": "wKtjDkin0cs1",
    "outputId": "83ffdc0b-ca16-4452-e23d-9daa81bc28f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Category URL : http://books.toscrape.com/catalogue/category/books/travel_2/index.html\n",
      "'NoneType' object has no attribute 'find'\n",
      "Finished.\n",
      "------------------------Category URL : http://books.toscrape.com/catalogue/category/books/childrens_11/index.html\n",
      "Requesting next page : http://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html\n",
      "'NoneType' object has no attribute 'find'\n",
      "Finished.\n",
      "------------------------Category URL : http://books.toscrape.com/catalogue/category/books/health_47/index.html\n",
      "'NoneType' object has no attribute 'find'\n",
      "Finished.\n",
      "------------------------Category URL : http://books.toscrape.com/catalogue/category/books/psychology_26/index.html\n",
      "'NoneType' object has no attribute 'find'\n",
      "Finished.\n",
      "------------------------Category URL : http://books.toscrape.com/catalogue/category/books/politics_48/index.html\n",
      "'NoneType' object has no attribute 'find'\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Initialize objects\n",
    "\n",
    "\n",
    "# Init lists\n",
    "Books_titles = []\n",
    "Books_categories = []\n",
    "Books_prices = []\n",
    "Books_availability = []\n",
    "Books_img = []\n",
    "Books_Url = []\n",
    "Books_descriptions = []\n",
    "Books_stars = []\n",
    "\n",
    "\n",
    "# Def categories\n",
    "categories = [\n",
    "    'travel',\n",
    "    'children',\n",
    "       'health',\n",
    "    'psychology',\n",
    "    'politics']\n",
    "\n",
    "# Init url\n",
    "base_url = \"http://books.toscrape.com\"\n",
    "url = base_url\n",
    "\n",
    "# open url\n",
    "response = requests.get(url)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# loop into all category names\n",
    "for category in categories :\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the category links in the main page\n",
    "    category_select = soup.select(f'ul.nav-list li a[href*=\"catalogue/category/books/{category}\"]')\n",
    "\n",
    "    # Extract and print the URLs\n",
    "    for cat in category_select:\n",
    "        # find category url\n",
    "        category_url = cat['href']\n",
    "        next_url = base_url + '/' + category_url\n",
    "        print(f\"------------------------Category URL : {next_url}\")\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "    # While Next button exists\n",
    "    while next_url:\n",
    "        # HTTP Request and BeautifulSoup\n",
    "        r = requests.get(next_url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        \n",
    "        \n",
    "        # find books and loop throught all of them\n",
    "        for book in soup.find_all('article', class_='product_pod'):\n",
    "            \n",
    "            # find the title\n",
    "            try:\n",
    "                title = book.h3.a['title']\n",
    "            except:\n",
    "                title = None\n",
    "                print('None - 1 ')\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "            # find the price\n",
    "            try:\n",
    "                price = book.find('p', class_='price_color').text\n",
    "            except:\n",
    "                price = None\n",
    "                print('None - 2 ')\n",
    "  \n",
    "            \n",
    "            # find the availability\n",
    "            try:\n",
    "                availability = book.find('p', class_='instock availability').text.strip()\n",
    "            except:\n",
    "                availability = None\n",
    "                print('None - 3 ')\n",
    "\n",
    " \n",
    "            # find the star rating\n",
    "            try : \n",
    "                star_rating_tag = book.find('p', class_='star-rating')\n",
    "                nb_star = star_rating_tag['class'][1]\n",
    "            except:\n",
    "                nb_star = None\n",
    "                print('None - 4 ')\n",
    "            \n",
    "            \n",
    "            # find the book url and nagivate in\n",
    "            try :\n",
    "                # book url\n",
    "                extracted_link = book.h3.a['href']\n",
    "                \n",
    "                # create the url to get in\n",
    "                book_url = base_url + '/catalogue/' + clean_url(extracted_link)\n",
    "                \n",
    "                # enter in the book page\n",
    "                r2 = requests.get(book_url)\n",
    "                soup2 = BeautifulSoup(r2.text, 'html.parser' )\n",
    "                \n",
    "                # find the descprition\n",
    "                description_ = soup2.find('meta', {'name': 'description'})\n",
    "                description = description_.get('content')\n",
    "\n",
    "                # find the image link\n",
    "                image_link = soup2.find('div', class_='item active').find('img')\n",
    "                image_url = image_link['src']\n",
    "            \n",
    "            except :\n",
    "                book_url = None\n",
    "                description = None\n",
    "                image_url = None\n",
    "                print('None - 5 ')\n",
    "                \n",
    "                \n",
    "            # store all informations in lists\n",
    "                \n",
    "            Books_categories.append(category)\n",
    "            \n",
    "            Books_titles.append(title)\n",
    "            \n",
    "            Books_prices.append(price)\n",
    "                \n",
    "            Books_availability.append(availability)\n",
    "                \n",
    "            Books_stars.append(nb_star)\n",
    "\n",
    "            Books_Url.append(book_url)\n",
    "            \n",
    "            Books_descriptions.append(description)\n",
    "            \n",
    "            Books_img.append(image_url)\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "          next_tag = soup.find('li', class_='next').find('a')['href']\n",
    "          next_url = urljoin(r.url, next_tag)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            next_url = None\n",
    "            \n",
    "            \n",
    "        sleep(0.2)\n",
    "        \n",
    "        print('Requesting next page : {}'.format(next_url) if next_url else 'Finished.')\n",
    "        \n",
    "        \n",
    "        \n",
    "# Store all the list in the dictionary\n",
    "Storage = {\"Books_titles\" : Books_titles, \n",
    "           \"Books_categories\" : Books_categories,\n",
    "           \"Books_prices\" : Books_prices,\n",
    "           \"Books_availability\" : Books_availability,\n",
    "           \"Books_img\" : Books_img,\n",
    "           \"Books_Url\" : Books_Url,\n",
    "           \"Books_descriptions\" : Books_descriptions,\n",
    "           \"Books_stars\" : Books_stars\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(Storage)\n",
    "\n",
    "\n",
    "# remove the '\\n'\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].replace('\\n', ' ', regex=True)\n",
    "    \n",
    "    \n",
    "# save the dataframe as a csv\n",
    "df.to_csv('books_scraped.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7e68FlTGJNoKqfAtTjhtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
